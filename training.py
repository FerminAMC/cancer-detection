# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colaboratory.

This file needs to be run in Google Colab
"""

# Project: Using CNNs and Transfer Learning to Identify Cancer Tumors
# Date: May 2nd, 2019
# Mentor: Benjamín Valdés Aguirre
# Coauthors: A01209923 Oscar Alberto Carreno Gutierrez
#            A01209665 Fermín Alejandro Moreno Cornejo

# This script implements a CNN as a mean to detect via two image classification models to detect certain features in
# a medical dataset that allows to detect Cancer via feature detection in a series of images.

# Sets the backend of matplotlib to the "inline backend"
# %matplotlib inline

# PIP Install additional packages for the CNN not included with the cloud notebook.
!pip install -q keras
!pip install kaggle
!pip install livelossplot

# Library Import
import numpy as np                                          # Numpy module as an alias (np) pointing to numpy.
import pandas as pd                                         # Pandas module as an alias (pd) pointing to pandas.
import os                                                   # OS module to interact with Windows and Anaconda.
import json                                                 # JSON module to utilize the file format with Python.
import random                                               # Random module for selection of evaluation files
import shutil                                               # Shutil module to execute high-level operations on files.
import zipfile                                              # .zip file module to manipulate compressed files in Python.
import matplotlib.pyplot as plt                             # Matplotlib to create figures in a MATLAB style.
from google.colab import drive, files                       # Allows to use and import data and mange files in Google's cloud filesystem.
from glob import glob                                       # Glob module to find pathnames matching a pattern.
from skimage.io import imread                               # Imread module to read and write images in different formats.
from sklearn.metrics import roc_curve, auc, roc_auc_score   # Sklearn metrics modules to plot characteristic curves.
from sklearn.model_selection import train_test_split        # Sklearn model modules to split arrays into random subsets.
from shutil import copyfile                                 # Shutil module to copy files
from livelossplot import PlotLossesKeras                    # Module to Live plot the Loss (Cost Function) while training.
from keras import backend                                   # Module to utilize a backend engine.
from keras.applications.nasnet import NASNetMobile          # Import the NASNet model for image classification.
from keras.applications.xception import Xception            # Import the Xception model for image classification.
from keras.callbacks import CSVLogger, ModelCheckpoint      # Modules to create a model checkpoint and log file of the network.
from keras.models import Model                              # Module to instantiate a Model's layers.
from keras.optimizers import Adam                           # Module to utilize Adam optimization (gradient-based).
from keras.preprocessing.image import ImageDataGenerator    # Generate Random Transformations and Normalization Operations for training.
from keras.utils.vis_utils import plot_model                # Utility Module to plot the network.

# NN Core Layer Modules from Keras:
#  - Average: Takes a list of tensors of the same shape and returns a single averaged tensor.
#  - Concatenate: Takes a list of tensors of the same shape and returns a single concatenated tensor.
#  - Dense: Module to use a regular interconnected layer of neutrons in a neural network.
#  - Dropout: Regularization module to reduce overfitting in classification and regression problems.
#  - Flatten: Module to remove multiple dimensions into one in the model.
#  - GlobalAveragePooling2D: Average Pooling operation for spatial data.
#  - GlobalMaxPooling2D: Max Pooling operation for spatial data.
#  - Input: Used to instatiate a Keras tensor.
from keras.layers import Average, Concatenate, Dense, Dropout, Flatten, GlobalAveragePooling2D, GlobalMaxPooling2D, Input

# Initialization for the Google Collaboration Environment, a free Jupyter cloud notebook with GPU support,
# The URL generated will create an authorization code to activate the notebook and assign the cloud resources,
# The notebook is mounted in the '/content/drive/' folder and is available with 12GB RAM & 358GB of storage.
drive.mount('/content/drive/')

# Directory of our project and where we will store the full program and data.
DIR_NAME = '/content/competitions/histopathologic-cancer-detection/'
DIR_SAMPLE = DIR_NAME + 'sample_submission.csv.zip/'        # Sample Submission zip file directory.
DIR_LABELS = DIR_NAME + 'train_labels.csv.zip/'             # Train Labels zip file directory.
DIR_TRAIN  = DIR_NAME + 'train/'                            # Train dataset directory.
DIR_TRAINF = DIR_NAME + 'train.zip/'                        # Train zip file directory.
DIR_TEST   = DIR_NAME + 'test/'                             # Test dataset directory.
DIR_TESTF  = DIR_NAME + 'test.zip/'                         # Test zip file directory.

# Searches if the path exists, if it doesn't exist it creates it, moves into the directory and prints the current working directory.
if not os.path.exists(DIR_NAME):
       os.makedirs(DIR_NAME)

os.chdir(DIR_NAME)
print(os.getcwd())

# Script to download the Histopathologic Cancer dataset from Kaggle into the working directory defined previously for the project.
# Insert your own kaggle keys
api_token = {"username":"xxxxxx","key":"xxxxxxxxxx"}
os.environ['KAGGLE_USERNAME'] = "xxxxxxx"
os.environ['KAGGLE_KEY'] = "xxxxxxxxxxxxxxx"
!kaggle config path -p /content
!kaggle competitions download -c histopathologic-cancer-detection

# Prints the full directory downloaded into the project folder with the .zip files from kaggle
print(os.listdir())

# Extracts the .zip compressed Sample Submission CSV file
FILE_NAME = os.path.abspath(DIR_SAMPLE)                     # Assigns the file name to the path of the .zip
ZIP_REF = zipfile.ZipFile(FILE_NAME)                        # Create the zipfile object of the sample file
ZIP_REF.extractall(DIR_NAME)                                # Extracts the file to the main directory
ZIP_REF.close()                                             # Close the zipfile object

# Extracts the .zip compressed Train Labels CSV file
FILE_NAME = os.path.abspath(DIR_LABELS)                     # Assigns the file name to the path of the .zip
ZIP_REF = zipfile.ZipFile(FILE_NAME)                        # Create the zipfile object of the sample file
ZIP_REF.extractall(DIR_NAME)                                # Extracts the file to the main directory
ZIP_REF.close()                                             # Close the zipfile object

# Searches if the train folder exists, if it doesn't exist it creates it.
if not os.path.exists(DIR_TRAIN):
    os.makedirs(DIR_TRAIN)

# Extracts the .zip compressed Train dataset
FILE_NAME = os.path.abspath(DIR_TRAINF)                     # Assigns the file name to the path of the .zip
ZIP_REF = zipfile.ZipFile(FILE_NAME)                        # Create the zipfile object of the train dataset
ZIP_REF.extractall(DIR_TRAIN)                               # Extracts the file to the main directory
ZIP_REF.close()                                             # Close the zipfile object

# Searches if the test folder exists, if it doesn't exist it creates it.
if not os.path.exists(DIR_TEST):
    os.makedirs(DIR_TEST)

# Extracts the .zip compressed Test dataset
FILE_NAME = os.path.abspath(DIR_TESTF)                      # Assigns the file name to the path of the .zip
ZIP_REF = zipfile.ZipFile(FILE_NAME)                        # Create the zipfile object of the test dataset
ZIP_REF.extractall(DIR_TEST)                                # Extracts the file to the main directory
ZIP_REF.close()                                             # Close the zipfile object

# Gets and prints the full directory downloaded into the project folder with the .zip files from kaggle
print(os.getcwd())
print(os.listdir())

# Final Output Files created to validate and log de results of the CNN
# All of them can be found in the folder containing the cancer_detector notebook

MODEL_FILE           = "model.h5"                         # Final model to be further loader in subsequential runs.
MODEL_PLOT_FILE      = "model_plot.png"                   # Exported image of the model core layers and their specs.
MODEL_SUMMARY_FILE   = "model_summary.txt"                # Summary text file of the model core layers and its specs.
TRAINING_LOGS_FILE   = "training_logs.csv"                # Log file of the training through time
TRAINING_PLOT_FILE   = "training.png"                     # Exported plot image of the loss through each epoch.
ROC_PLOT_FILE        = "roc.png"                          # Exported image of the ROC curve (true positive rate against false positives).
VALIDATION_PLOT_FILE = "validation.png"                   # Exported plot image of the accuracy through each epoch.

# Hyperparameters for the Neural Network, can be modified to improve time & performance.

BATCH_SIZE         = 192                                    # Mini Batch Size
EPOCHS             = 8                                      # Number of epochs that will run the complete CNN
IMAGE_SIZE         = 96                                     # Image Pixel size of the dataset
LEARNING_SIZE      = 0.9                                    # Learning size represents the percentage of images that will be used for training and validation
SAMPLE_COUNT       = 85000                                  # Total number of samples
TESTING_BATCH_SIZE = 5000                                   # Batch Size for the Test set
VERBOSITY          = 1                                      # Mode to display all of the terminal results.

# Data setup to be utilized for the network
data_frame = pd.DataFrame({'path': glob(os.path.join(DIR_TRAIN,'*.tif'))})      # Creates a DataFrame data structure that contains all the names of the training set
data_frame['id'] = data_frame.path.map(lambda x: x.split('/')[5].split('.')[0]) # Stores the name of all the .tif files in the DataFrame
labels = pd.read_csv(DIR_NAME + '/train_labels.csv')                            # Stores all labels for the training set
data_frame = data_frame.merge(labels, on = 'id')                                # Merges all labels with the id's on the Data Frame
negatives = data_frame[data_frame.label == 0].sample(SAMPLE_COUNT)              # Takes random samples for the negative portion of the set
positives = data_frame[data_frame.label == 1].sample(SAMPLE_COUNT)              # Takes random samples for the positive portion of the set
data_frame = pd.concat([negatives, positives]).reset_index()                    # Concatenates the negative and positive samples, creating a new DataFrame
data_frame = data_frame[['path', 'id', 'label']]                                # New names for each of the columns in the DataFrame
data_frame['image'] = data_frame['path'].map(imread)                            # New column for the DataFrame with the images in the DataFrame

# Paths for training and validation directories
training_path = '../training'
validation_path = '../validation'

# Creates training and validation folders that contain the images with 1's and 0's stored in the DataFrame
for folder in [training_path, validation_path]:
    for subfolder in ['0', '1']:
        path = os.path.join(folder, subfolder)
        os.makedirs(path, exist_ok=True)

training, validation = train_test_split(data_frame, train_size=LEARNING_SIZE, stratify=data_frame['label'])

data_frame.set_index('id', inplace=True)

for images_and_path in [(training, training_path), (validation, validation_path)]:
    images = images_and_path[0]
    path = images_and_path[1]
    for image in images['id'].values:
        file_name = image + '.tif'
        label = str(data_frame.loc[image,'label'])
        destination = os.path.join(path, label, file_name)
        if not os.path.exists(destination):
            source = os.path.join(DIR_NAME + 'train', file_name)
            shutil.copyfile(source, destination)

# Data augmentation: every image that passes throught this data generator will be augmented in the following ways
training_data_generator = ImageDataGenerator(
                                              channel_shift_range = 0.3,
                                              height_shift_range  = 0.3,
                                              horizontal_flip     = True,
                                              rescale             = 1./255,
                                              rotation_range      = 180,
                                              shear_range         = 0.3,
                                              vertical_flip       = True,
                                              width_shift_range   = 0.3,
                                              zoom_range          = 0.4
                                            )

# Data generation: the images are passed through the data generator created in the previous step
# The newly generated batches of images are stored in the corresponding folders
training_generator = training_data_generator.flow_from_directory( training_path,
                                                                  target_size = (IMAGE_SIZE,IMAGE_SIZE),
                                                                  batch_size  = BATCH_SIZE,
                                                                  class_mode  = 'binary')

validation_generator = ImageDataGenerator(rescale = 1./255).flow_from_directory( validation_path,
                                                                                 target_size = (IMAGE_SIZE,IMAGE_SIZE),
                                                                                 batch_size  = BATCH_SIZE,
                                                                                 class_mode  = 'binary')

testing_generator = ImageDataGenerator(rescale = 1./255).flow_from_directory( validation_path,
                                                                              target_size = (IMAGE_SIZE,IMAGE_SIZE),
                                                                              batch_size  = BATCH_SIZE,
                                                                              class_mode  = 'binary',
                                                                              shuffle     = False)

# Model
input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)                                       # Defines the model input shape with a size of 96 x 96 x 3 (RGB).
inputs = Input(input_shape)                                                     # Creates the input shape objects.

xception = Xception(include_top = False, input_shape = input_shape)(inputs)     # Set as false since it uses a different size matrix than the original
nas_net = NASNetMobile(include_top=False, input_shape=(224,224,3))(inputs)  # one and adds the input shape designed for our dataset.

outputs = Concatenate(axis=-1)([GlobalAveragePooling2D()(xception),             # Takes a list of the two models used of the same shape and returns a single
                                GlobalAveragePooling2D()(nas_net)])             # concatenated model while averaging the pooling operation for spatial data.
outputs = Dropout(0.5)(outputs)                                                 # Regularize the data to reduce overfitting in our model.
outputs = Dense(1, activation='sigmoid')(outputs)

model = Model(inputs, outputs)     # Keras creates a model with all the necessary layers required in the computation of outputs given inputs
# model.compile is the configuration for the model to be trained with.
# In this case, we chose the Adam optimizer, which is an extension of stochastic gradient descent that is used for computer vision and natural language
#lr=0.0001, decay=0.00001
model.compile(optimizer=Adam(lr=0.01, decay=0.01),
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.summary()

plot_model(model,
           to_file=MODEL_PLOT_FILE,
           show_shapes=True,
           show_layer_names=True)



# Or run this block for the Training cell, will use the settings and hyperparameter defined previously

history = model.fit_generator(training_generator,
                              steps_per_epoch = len(training_generator),
                              validation_data = validation_generator,
                              validation_steps = len(validation_generator),
                              epochs    = EPOCHS,
                              verbose   = VERBOSITY,
                              callbacks = [PlotLossesKeras(),
                                           ModelCheckpoint( MODEL_FILE,
                                                            mode = 'max',
                                                            monitor = 'val_acc',
                                                            save_best_only = True,
                                                            verbose = VERBOSITY
                                                          ),
                                           CSVLogger( TRAINING_LOGS_FILE,
                                                      append = False,
                                                      separator = ';')])


# Training plots
epochs = [i for i in range(1, len(history.history['loss'])+1)]

plt.plot(epochs, history.history['loss'], color='blue', label="training_loss")
plt.plot(epochs, history.history['val_loss'], color='red', label="validation_loss")
plt.legend(loc='best')
plt.title('training')
plt.xlabel('epoch')
plt.savefig(TRAINING_PLOT_FILE, bbox_inches='tight')
plt.close()

plt.plot(epochs, history.history['acc'], color='blue', label="training_accuracy")
plt.plot(epochs, history.history['val_acc'], color='red',label="validation_accuracy")
plt.legend(loc='best')
plt.title('validation')
plt.xlabel('epoch')
plt.savefig(VALIDATION_PLOT_FILE, bbox_inches='tight')
plt.close()


model.load_weights(MODEL_FILE)
predictions = model.predict_generator(testing_generator, steps=len(testing_generator), verbose=VERBOSITY)
false_positive_rate, true_positive_rate, threshold = roc_curve(testing_generator.classes, predictions)
area_under_curve = auc(false_positive_rate, true_positive_rate)

plt.plot([0, 1], [0, 1], 'k--')
plt.plot(false_positive_rate, true_positive_rate, label='AUC = {:.3f}'.format(area_under_curve))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve')
plt.legend(loc='best')
plt.savefig(ROC_PLOT_FILE, bbox_inches='tight')
plt.close()

!ls -la


files.download('/content/competitions/histopathologic-cancer-detection/model.h5')
files.download('/content/competitions/histopathologic-cancer-detection/model_plot.png')
files.download('/content/competitions/histopathologic-cancer-detection/roc.png')
files.download('/content/competitions/histopathologic-cancer-detection/sample_submission.csv')
files.download('/content/competitions/histopathologic-cancer-detection/training_logs.csv')
